{"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7939268,"sourceType":"datasetVersion","datasetId":4667545},{"sourceId":7945443,"sourceType":"datasetVersion","datasetId":4671853},{"sourceId":7947158,"sourceType":"datasetVersion","datasetId":4673114}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/smellytofu/delfttreescnn?scriptVersionId=168847286\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#to clear outputs\n\nimport os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T11:33:56.686113Z","iopub.execute_input":"2024-03-26T11:33:56.686822Z","iopub.status.idle":"2024-03-26T11:33:56.729167Z","shell.execute_reply.started":"2024-03-26T11:33:56.686786Z","shell.execute_reply":"2024-03-26T11:33:56.727872Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.optimizers import RMSprop\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport os\nimport scipy\n\n\nimg = image.load_img(\"/kaggle/input/tree-type-classification-main/tree-type-classification-main/Model/is_tree_images/train_images/tree/152_2021-09_52-009821571910685_4-325297593030579_Redelijk.jpg\")\nplt.imshow(img)\ncv2.imread(\"/kaggle/input/tree-type-classification-main/tree-type-classification-main/Model/is_tree_images/train_images/tree/152_2021-09_52-009821571910685_4-325297593030579_Redelijk.jpg\").shape\n\ntrain = ImageDataGenerator(rescale= 1/255)\nvalidation = ImageDataGenerator(rescale= 1/255)\n\ntrain_dataset = train.flow_from_directory(\"/kaggle/input/tree-type-classification-main/tree-type-classification-main/Model/is_tree_images/train_images\", target_size = (200,200), batch_size = 3, class_mode = 'binary')\n\nvalidate_dataset = validation.flow_from_directory(\"/kaggle/input/tree-type-classification-main/tree-type-classification-main/Model/is_tree_images/validate_images\",target_size = (200,200), batch_size = 3, class_mode = 'binary')\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Flatten(),\n\n    tf.keras.layers.Dense(512, activation='relu'),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n])\nmodel.compile(loss='binary_crossentropy',\n              optimizer = RMSprop(learning_rate=0.001),\n              metrics = ['accuracy'])\nmodel_fit = model.fit(train_dataset,\n                      steps_per_epoch = 20,\n                      epochs = 30,\n                      validation_data = validate_dataset)\n\nimg = image.load_img(\"/kaggle/input/tree-type-classification-main/tree-type-classification-main/Model/is_tree_images/validate_images/tree/13182_2021-09_52-01082266294491_4-355730754059023_Slecht.jpg\", target_size=(200,200))\nplt.imshow(img)\nplt.show()\n\nX = image.img_to_array(img)\nX = np.expand_dims(X,axis = 0)\nimages = np.vstack([X])\nval = model.predict(images)\nif val == 0:\n    print(\"not a tree\")\nelse:\n    print(\"a tree\")\ntf.keras.models.save_model(model,\"/kaggle/working/is_tree_model.keras\", 'is_tree_model')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T12:44:31.956734Z","iopub.execute_input":"2024-03-26T12:44:31.957257Z","iopub.status.idle":"2024-03-26T12:44:44.141323Z","shell.execute_reply.started":"2024-03-26T12:44:31.957222Z","shell.execute_reply":"2024-03-26T12:44:44.139439Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#classify tree/notree\n\nfrom pathlib import Path \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image  # Importing from tf.keras\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport shutil\nloaded_model = tf.keras.models.load_model(\"/kaggle/working/is_tree_model.keras\")\nTREE_TYPES = ['Dood', 'Goed', 'Matig', 'Redelijk', 'Slecht', 'Zeer Slecht']\n\nPath('/kaggle/working/classified_images/').mkdir(parents=True, exist_ok=True)\n\nFOLDER_PATH_OUTPUT = '/kaggle/working/classified_images/'\nFOLDER_PATH_INPUT = '/kaggle/input/trees-by-category/images/'\n\nfor tree_path in TREE_TYPES:   \n# Construct the full path for the current tree type\n    output_tree = os.path.join(FOLDER_PATH_OUTPUT, 'tree', tree_path)\n    output_not_tree = os.path.join(FOLDER_PATH_OUTPUT, 'not_tree', tree_path)\n\n\n\n    # Check if the directory exists, if not, create it\n    if not os.path.exists(output_tree):\n        os.makedirs(output_tree)\n\n\n    if not os.path.exists(output_not_tree):\n        os.makedirs(output_not_tree)\n    \n\nfor tree_path in TREE_TYPES:\n    # Construct the full path for the current tree type\n    output_tree = os.path.join(FOLDER_PATH_OUTPUT, 'tree', tree_path)\n    output_not_tree = os.path.join(FOLDER_PATH_OUTPUT, 'not_tree', tree_path)\n    input_tree_type = os.path.join(FOLDER_PATH_INPUT, tree_path)\n\n    for filename in os.listdir(input_tree_type):\n        image_input_path = os.path.join(input_tree_type, filename)\n        if os.path.isfile(image_input_path):\n            # Load image using TensorFlow's Keras preprocessing\n            img = image.load_img(image_input_path, target_size=(200, 200))\n\n            # Convert image to array\n            X = image.img_to_array(img)\n            X = np.expand_dims(X, axis=0)\n\n            # Predict\n            prediction = loaded_model.predict(X)\n\n            if prediction == 0:\n                shutil.copy(image_input_path, os.path.join(output_not_tree, filename))\n            else:\n                shutil.copy(image_input_path, os.path.join(output_tree, filename))\n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#blurring images \n!pip install inference_sdk\nfrom inference_sdk import InferenceHTTPClient, InferenceConfiguration\nfrom PIL import Image, ImageDraw, ImageFilter\nimport os\n\ncustom_configuration = InferenceConfiguration(confidence_threshold=0.1)\n\nCLIENT = InferenceHTTPClient(\n    api_url=\"https://outline.roboflow.com\",\n    api_key=\"kiW8upICXaWRP7kAm4Ct\"\n)\n\n# Define the root folder where your images are stored\nroot_folder = \"/kaggle/working/classified_images/tree\"\n\n# Iterate over each subfolder\nfor tree_cat in os.listdir(root_folder):\n    tree_cat_path = os.path.join(root_folder, tree_cat)\n\n\n    # Iterate over each image in the \"tree\" folder\n    for img_file in os.listdir(tree_cat_path):\n            img_path = os.path.join(tree_cat_path, img_file)\n\n            with CLIENT.use_configuration(custom_configuration):\n                results = CLIENT.infer(img_path, model_id=\"cameraexplorer/1\")\n\n            # Load the original image\n            image = Image.open(img_path)\n\n            # Create a blurred version of the original image\n            blurred_image = image.filter(ImageFilter.GaussianBlur(15))  # Adjust the blur radius as needed\n\n            # Initialize a blank mask with the same dimensions as the original image\n            mask = Image.new('L', image.size, 0)\n            draw = ImageDraw.Draw(mask)\n\n            has_tree = False  # Initialize flag\n\n            # Iterate over each detected object in your results\n            for obj in results['predictions']:\n                if obj['class'] == 'tree':\n                    has_tree = True  # Initialize flag\n                    # 'points' is a list of dictionaries, each with 'x' and 'y' keys\n                    # Convert this into a list of tuples that PIL can use to draw the polygon\n                    polygon = [(point['x'], point['y']) for point in obj['points']]\n\n                    # Draw the polygon on the mask and fill it with white (255)\n                    draw.polygon(polygon, fill=255)\n\n            # Combine the original image with the blurred image, using the mask to keep the areas within the polygons clear\n            final_image = Image.composite(image, blurred_image, mask)\n\n            # Save\n            if has_tree:\n                final_image.save(img_path)\n                print(f'Saving {img_path} in {tree_cat}')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T12:15:51.500264Z","iopub.execute_input":"2024-03-26T12:15:51.50064Z","iopub.status.idle":"2024-03-26T12:15:51.554419Z","shell.execute_reply.started":"2024-03-26T12:15:51.500611Z","shell.execute_reply":"2024-03-26T12:15:51.553321Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transform and generate images for low sample datasets (after importing blurred_trees)\nimport os\nfrom collections import Counter\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport random\n\n# Function to identify categories with lower samples\ndef identify_low_sample_categories(images_folder):\n    image_counts = {}\n    for cat_folder in os.listdir(images_folder):\n        cat_path = os.path.join(images_folder, cat_folder)\n        image_counts[cat_folder] = len(os.listdir(cat_path))\n    max_samples = max(image_counts.values())\n    low_sample_categories = [label for label, count in image_counts.items() if count < max_samples]\n    return low_sample_categories\n\n# Define data augmentation transforms\naugmentation_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2))  # Random translation\n])\n\n# Function to apply transformations and save additional images\ndef augment_and_save_images(images_folder, low_sample_categories, transforms):\n\n    image_counts = {}\n    for cat_folder in os.listdir(images_folder):\n        cat_path = os.path.join(images_folder, cat_folder)\n        image_counts[cat_folder] = len(os.listdir(cat_path))\n        \n    cat_counts = Counter(image_counts)\n    max_samples = max(cat_counts.values())\n    for category in low_sample_categories:\n        os.makedirs('/kaggle/working/'+\"tree\"+\"/\"+category)\n        cat_folder = os.path.join(images_folder, category)\n        num_samples = cat_counts[category]\n\n        # Calculate how many additional images to generate\n        num_additional_images = max_samples - num_samples\n        for i in range (num_additional_images):\n            random_index = random.randint(0, len(os.listdir(cat_folder)) - 1)\n            randomfilename=os.listdir(cat_folder)[random_index]\n            image_path = os.path.join(cat_folder, randomfilename)\n            image = Image.open(image_path)\n\n            # Apply transformations and save additional images\n            transformed_image = transforms(image)\n            new_filename = f\"augmented_{num_samples + i + 1}_{randomfilename}\"  # Append prefix to filename\n            new_image_path = os.path.join(\"/kaggle/working/\"+\"tree/\"+category, new_filename)\n            transformed_image.save(new_image_path)\n# Transform and generate new blurred images for categories with smaller samples\nimages_folder = '/kaggle/input/blurred-trees/classified_images/tree'\nlow_sample_categories = identify_low_sample_categories(images_folder)\naugment_and_save_images(images_folder, low_sample_categories, augmentation_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T11:34:00.690099Z","iopub.execute_input":"2024-03-26T11:34:00.690487Z","iopub.status.idle":"2024-03-26T11:35:08.448004Z","shell.execute_reply.started":"2024-03-26T11:34:00.690448Z","shell.execute_reply":"2024-03-26T11:35:08.446511Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn.functional as F\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom transformers import ViTForImageClassification\nfrom scipy import stats\nfrom PIL import Image, ImageDraw, ImageFilter\n\nlabel_to_int = {'Dood': 0, 'Goed': 1, 'Matig': 2, 'Redelijk': 3, 'Slecht': 4,'Zeer Slecht': 5 }\n\nclass CustomImageDataset1(Dataset):\n    def __init__(self, images, labels, transforms=None, samples_per_class=None):\n        self.images = images\n        self.labels = [label_to_int[label] for label in labels]\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = Image.open(self.images[idx])\n        label = self.labels[idx]\n\n        # Remove top 30% of the image\n        width, height = image.size\n        image = image.crop((0, height * 0.3, width, height))\n\n        if self.transforms:\n            image = self.transforms(image)\n\n        return image, label\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, images, labels, transforms=None, sampling_factors=None):\n        self.images = images\n        self.labels = labels\n        self.transforms = transforms\n        self.sampling_factors = sampling_factors if sampling_factors is not None else {label: 1 for label in set(labels)}\n\n    def __len__(self):\n        # Calculate total length considering sampling factors\n        total_length = 0\n        for label in self.labels:\n            total_length += self.sampling_factors[label]\n        return total_length\n\n    def __getitem__(self, idx):\n        # Find the actual image index based on the sampling factor\n        actual_idx = idx\n        for i, label in enumerate(self.labels):\n            if actual_idx < self.sampling_factors[label]:\n                break\n            actual_idx -= self.sampling_factors[label]\n\n\n        image = Image.open(self.images[i])\n        label = self.labels[i]\n\n        # Remove top 30% of the image\n        width, height = image.size\n        image = image.crop((0, height * 0.3, width, height))\n\n        # Process image\n        if self.transforms:\n            image = self.transforms(image)\n\n        return image, label_to_int[label]\n\ndef load_images(folder_path):\n    print(\"Loading images...\")\n    images = []\n    labels = []\n    for class_folder in os.listdir(folder_path):\n        class_path = os.path.join(folder_path, class_folder)\n        for img_file in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_file)\n            images.append(img_path)\n            labels.append(class_folder)\n    print(\"Total images loaded:\", len(images))\n    return images, labels\n\ndef split_dataset(images, labels, train_ratio=0.6, val_ratio=0.2):\n    # Split dataset into train, validation, and test\n    print(\"Splitting dataset...\")\n    dataset = list(zip(images, labels))\n    random.shuffle(dataset)\n    train_size = int(len(dataset) * train_ratio)\n    val_size = int(len(dataset) * val_ratio)\n    train_set = dataset[:train_size]\n    val_set = dataset[train_size:train_size + val_size]\n    test_set = dataset[train_size + val_size:]\n    print(f\"Dataset split into {len(train_set)} training, {len(val_set)} validation, and {len(test_set)} test images.\")\n    return train_set, val_set, test_set\n\ndef count_class_distribution(dataset):\n    class_counts = {}\n    for _, label in dataset:\n        class_counts[label] = class_counts.get(label, 0) + 1\n    return class_counts\n\ndef calculate_sampling_factors(train_set):\n    label_counts = Counter(label for _, label in train_set)\n    min_samples = min(label_counts.values())\n    print(min_samples)\n\n    # Calculate sampling factor for each class\n    sampling_factors = {label: round(min_samples / count) for label, count in label_counts.items()}\n    return sampling_factors\n\ndef create_dataloaders(train_set, val_set, test_set, sampling_factors, batch_size):\n    # Define transformations\n    print(\"Creating dataloaders...\")\n\n    train_transforms = transforms.Compose([\n        transforms.RandomCrop(224),  # Specify size\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    val_test_transforms = transforms.Compose([\n        transforms.RandomCrop(224),  # Specify size\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n\n    train_dataset = CustomImageDataset([i[0] for i in train_set], [i[1] for i in train_set], transforms=train_transforms, sampling_factors=sampling_factors)\n    val_dataset = CustomImageDataset([i[0] for i in val_set], [i[1] for i in val_set], transforms=val_test_transforms)\n    test_dataset = CustomImageDataset([i[0] for i in test_set], [i[1] for i in test_set], transforms=val_test_transforms)\n    # print('Dataset', train_dataset[0])\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    print(\"Dataloaders created.\")\n    print(len(train_loader), len(val_loader), len(test_loader))\n    return train_loader, val_loader, test_loader\n\ndef calculate_samples_per_class(class_distribution):\n    min_count = min(class_distribution.values())\n    samples_per_class = {label_to_int[cls]: min_count // count for cls, count in class_distribution.items()}\n    return samples_per_class\n\n\n\ndef load_data(folder1):\n    images, labels = load_images(folder1)\n    train_set, val_set, test_set = split_dataset(images, labels)\n\n    # Print class distribution\n    train_class_distribution = count_class_distribution(train_set)\n    val_class_distribution = count_class_distribution(val_set)\n    test_class_distribution = count_class_distribution(test_set)\n\n    print(\"Training set class distribution:\", train_class_distribution)\n    print(\"Validation set class distribution:\", val_class_distribution)\n    print(\"Test set class distribution:\", test_class_distribution)\n    sampling_factors = {'Matig': 1, 'Redelijk': 1, 'Slecht': 1, 'Dood': 1, 'Goed': 1, 'Zeer Slecht': 1}\n    \n\n#     sampling_factors = calculate_sampling_factors(train_set)\n    print('Sampling factors: ', sampling_factors)\n    train_loader, val_loader, test_loader = create_dataloaders(train_set, val_set, test_set, sampling_factors, batch_size=8)\n\n    return train_loader, val_loader, test_loader","metadata":{"id":"HWZBGOIC_EDS","execution":{"iopub.status.busy":"2024-03-26T12:50:00.513049Z","iopub.execute_input":"2024-03-26T12:50:00.513461Z","iopub.status.idle":"2024-03-26T12:50:00.548492Z","shell.execute_reply.started":"2024-03-26T12:50:00.513429Z","shell.execute_reply":"2024-03-26T12:50:00.547598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_patches(image, patch_size, stride):\n    patches = []\n    c, height, width = image.size()\n\n    for y in range(0, height - patch_size[1] + 1, stride):\n        for x in range(0, width - patch_size[0] + 1, stride):\n            patch = image[:, y:y + patch_size[1], x:x + patch_size[0]]\n            patches.append(patch)\n\n    return patches\n\ndef train_one_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    total_samples = 0\n    all_labels = []\n    all_preds = []\n\n    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n    for images, labels in progress_bar:\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n\n        # Forward pass\n        outputs = model(images)\n\n        #For ViT\n#         logits = outputs.logits  # Extract the logits\n#         loss = criterion(logits, labels)\n\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Statistics\n        # _, predicted = torch.max(outputs, 1)\n        _, predicted = torch.max(outputs, 1)\n\n        correct = (predicted == labels).sum().item()\n        progress_bar.set_postfix(loss=loss.item(), accuracy=correct/len(labels))\n\n        total_loss += loss.item()\n        total_correct += correct\n        total_samples += labels.size(0)\n\n        # For F1 score calculation\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(predicted.cpu().numpy())\n\n    avg_loss = total_loss / len(train_loader)\n    accuracy = total_correct / total_samples\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    per_class_f1 = f1_score(all_labels, all_preds, average=None)\n\n    return avg_loss, accuracy, f1, per_class_f1\n\n\n\ndef validate_or_test(model, loader, criterion, device, patch_size, stride, desc='Val'):\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    all_labels = []\n    all_preds = []\n\n    progress_bar = tqdm(loader, desc=desc, leave=False)\n    with torch.no_grad():\n        for images, labels in progress_bar:\n            images = images.to(device)\n            labels = labels.to(device)\n            batch_preds = []\n\n            for image in images:\n                # Apply sliding window approach\n                patches = extract_patches(image, patch_size, stride)\n                patches = torch.stack(patches).to(device)\n\n                # Aggregate predictions for each patch\n                patch_outputs = model(patches)\n\n                # Calculate mode for each patch prediction\n                modes, _ = torch.mode(patch_outputs, dim=0)\n                batch_preds.append(modes)\n\n            batch_preds = torch.stack(batch_preds)\n            loss = criterion(batch_preds, labels)\n            total_loss += loss.item()\n\n            _, predicted = torch.max(batch_preds, 1)\n            total_samples += labels.size(0)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n            # Update progress bar\n            progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    per_class_f1 = f1_score(all_labels, all_preds, average=None)\n\n    conf_matrix = confusion_matrix(all_labels, all_preds)\n    conf_matrix_percentage = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n\n    # Plot confusion matrix\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(conf_matrix_percentage, annot=True, fmt='g', cmap='Blues')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()\n    return avg_loss, accuracy, f1, per_class_f1\n\ndef train_and_evaluate(model, train_loader, val_loader, test_loader, model_name, num_epochs, patch_size=(224, 224), stride=30):\n    # Criterion, Optimizer, and Scheduler\n    weight_decay=0.01 #L2 Optimisation\n    lr=2e-4\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n    global best_val_f1\n    global best_model_weights\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        train_loss, train_accuracy, train_f1, train_f1_per_class = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1 Score: {train_f1:.4f}')\n        print(f'Train F1 Score Per Class ', train_f1_per_class)\n\n        val_loss, val_accuracy, val_f1, val_f1_per_class = validate_or_test(model, val_loader, criterion, device, patch_size, stride, desc='Val')\n        print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}')\n        print(f'Val F1 Score Per Class ', val_f1_per_class)\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_model_weights = model.state_dict()  # Save the best model weights\n\n        # Save intermediate model weights\n        torch.save(model.state_dict(), '/kaggle/working/model_state.pth')\n\n        scheduler.step()\n\n\n    test_loss, test_accuracy, test_f1, test_f1_per_class = validate_or_test(model, test_loader, criterion, device, patch_size, stride, desc='Test')\n    print(f'Test Loss: {test_loss:.3f}, Accuracy: {test_accuracy:.3f}, F1 Score: {test_f1:.3f}, F1 Score Per Class [{test_f1_per_class[0]:.3f}')\n    print(f'Test F1 Score Per Class ', test_f1_per_class)\n\n    # After training is complete, load the best model weights\n    model.load_state_dict(best_model_weights)\n\n    # Save the best model weights\n    torch.save(model.state_dict(), '/kaggle/working/best_model_state.pth')\n    return model","metadata":{"id":"2Iz78yn6_S65","execution":{"iopub.status.busy":"2024-03-26T12:50:00.903155Z","iopub.execute_input":"2024-03-26T12:50:00.903587Z","iopub.status.idle":"2024-03-26T12:50:00.933335Z","shell.execute_reply.started":"2024-03-26T12:50:00.903556Z","shell.execute_reply":"2024-03-26T12:50:00.932314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the main function with the path to folder1 (after appending augmentated datasets to orginal dataset --final dataset called blurred-trees_withdataaugmentation)\nimagesRoot = '/kaggle/input/blurred-trees-withdataaugmentation/tree'\n\npath_to_images = imagesRoot \ntrain_loader, val_loader, test_loader = load_data(path_to_images)\n\n# def load_pretrained_vit(num_labels):\n#  model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in2\nclass CNNWithFC(nn.Module):\n    def __init__(self, num_classes):\n        super(CNNWithFC, self).__init__()\n        # Define convolutional layers\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n#         self.conv3= nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\n        \n        # Calculate input size for fully connected layers\n        self._calculate_conv_output_size()\n        \n        # Define fully connected layers\n        self.fc1 = nn.Linear(self.fc_input_size, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        # Apply convolutional layers\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, kernel_size=2, stride=2)\n#         x = F.relu(self.conv3(x))\n#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n        # Flatten the feature map\n        x = x.view(x.size(0), -1)\n        \n        # Apply fully connected layers\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n    def _calculate_conv_output_size(self):\n        # Forward pass to calculate the output size after convolutional layers\n        with torch.no_grad():\n            x = torch.zeros(1, 3, 224, 224)  # Assuming input image size is 224x224\n            x = self.conv1(x)\n            x = F.max_pool2d(x, kernel_size=2, stride=2)\n            x = self.conv2(x)\n            x = F.max_pool2d(x, kernel_size=2, stride=2)\n#             x = self.conv3(x)\n#             x = F.max_pool2d(x, kernel_size=2, stride=2)\n            self.fc_input_size = x.size(1) * x.size(2) * x.size(3)\ndef load_pretrained_cnn_with_fc(num_labels):\n    # Define the CNN architecture\n        # Instantiate the CNN model with fully connected layers\n    model = CNNWithFC(num_labels)\n    return model\nnum_classes = 6  \nnum_epochs = 5\n# Load the pretrained ResNet-50 model\n# model = models.resnet50(pretrained=True)\n# vit_model = load_pretrained_vit(num_labels=num_classes)\ncnn_model = CNNWithFC(num_classes=num_classes)\n\n# model.fc = nn.Linear(model.fc.in_features, num_classes)\n# Initialize best F1 score for validation\n\nbest_val_f1 = 0.0\nbest_model_weights = None\nmodel_name = 'CNNwithFC'\ntrained_model = train_and_evaluate(cnn_model, train_loader, val_loader, test_loader, model_name, num_epochs)","metadata":{"id":"QsA9rp76_V9U","execution":{"iopub.status.busy":"2024-03-26T12:50:01.322965Z","iopub.execute_input":"2024-03-26T12:50:01.323953Z","iopub.status.idle":"2024-03-26T12:55:08.080165Z","shell.execute_reply.started":"2024-03-26T12:50:01.323909Z","shell.execute_reply":"2024-03-26T12:55:08.079161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}